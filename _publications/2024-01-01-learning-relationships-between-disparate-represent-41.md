---
title: "Learning Relationships Between Disparate Representations of Objects with Transformers and Contrastive Losses"
collection: publications
permalink: /publication/2024-learning-relationships-between-disparate-represent
excerpt: '[<u><span style="color:blue">Google Scholar</span></u>](https://scholar.google.com/scholar?q=Learning+Relationships+Between+Disparate+Representations+of+Objects+with+Transformers+and+Contrastive+Losses)'
date: 2024-01-01
venue: 'Authorea Preprints'
paperurl: 'https://www.authorea.com/doi/full/10.22541/essoar.172675995.55091022'
citation: 'Nesar Ramachandra and Azton Wells and Nick Frontiere and Salman Habib (2024). "Learning Relationships Between Disparate Representations of Objects with Transformers and Contrastive Losses". Authorea Preprints.'
---

Summary: Large language models are moving into a multi-modal space covering text, images, audio, and other media, yet scientific datasets possess features that are not represented by this paradigm. In cosmology, for instance, galaxies are embedded within dark matter-dominated collapsed objects, termed halos. A galaxy can be described in many ways: star formation history (SFH) characterizing the evolution of stellar content, magnitudes describing luminosity across different wavelengths (MAG), images, or matter distribution of the hosting halo. Despite their differences, these descriptions represent the same cosmological object. We propose the Object Foundation Model (OFM) to learn disparate representations of objects in scientific domains. OFM learns the general underlying representation of sparse and indeterminate data and facilitates robust predictions on diverse inputs. Unlike traditional deep learning with â€¦
