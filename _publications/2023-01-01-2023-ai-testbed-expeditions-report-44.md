---
title: "2023 AI Testbed Expeditions Report"
collection: publications
permalink: /publication/2023-2023-ai-testbed-expeditions-report
excerpt: '[<u><span style="color:blue">Google Scholar</span></u>](https://scholar.google.com/scholar?q=2023+AI+Testbed+Expeditions+Report)'
date: 2023-01-01
venue: 'Preprint'
paperurl: 'https://www.osti.gov/servlets/purl/2439992'
citation: 'Venkat Vishwanath and Murali Emani and Varuni Sastry and William Arnold and Rajeev Thakur and Valerie Taylor and Ian Foster and Salman Habib and Michael E Papka and Bryce Allen and Henry Chan and Rodrigo Ceccato de Freitas and Mathew J Cherukara and Miaoqi Chu and Jose M Monsalve Diaz and Neil Getty and Ross Harder and Kyle Hippe and Saugat Kandel and Antonino Miceli and Suresh Narayanan and Oleksandr Narykov and Alexander Partin and Nesar Ramachandra and Arvind Ramanathan and Esteban Rangel and Siddhisanket Raskar and Andrew Siegel and John Tramm and Thomas Uram and Azton Wells and Leighton Wilson and Fangfang Xia and Kazutomo Yoshii and Ruoxi Zhao and Tao Zhou (2023). "2023 AI Testbed Expeditions Report". Preprint.'
---

The recent trend in computing toward deep learning has resulted in the development of a variety of highly innovative AI accelerator architectures. One such architecture, the Cerebras Wafer-Scale Engine 2 (WSE-2), features 40 GB of on-chip SRAM, making it an attractive platform for latency-or bandwidth-bound HPC simulation workloads. In this study we examine the feasibility of performing continuous energy Monte Carlo (MC) particle transport by porting a key kernel from the MC transport algorithm to Cerebras’s CSL programming model. We then optimize the kernel and experiment with several novel algorithms for decomposing data structures across the WSE-2’s 2D network grid of approximately 750,000 user-programmable distributedmemory compute cores and for flowing particles (tasks) through the WSE-2’s network for processing. New algorithms for minimizing communication costs and for handling load balancing are developed and tested. The WSE-2 is found to run 130 times faster than a highly optimized CUDA version of the kernel run on an NVIDIA A100 GPU—significantly outpacing the expected performance increase given the relative number of transistors each architecture has.
