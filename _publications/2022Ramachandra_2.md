---
title: "Interpretable Uncertainty Quantification in AI for HEP"
collection: publications
permalink: /publication/2022Ramachandra_2
excerpt: '[<u><span style="color:blue"> arXiv link </span></u>](https://arxiv.org/abs/2208.03284)'
date: 2022
venue: 'Submitted to the Proceedings of the US Community Study on the Future of Particle Physics (Snowmass 2021)'
paperurl: 'https://doi.org/10.48550/arXiv.2208.03284'
citation: 'Thomas Y. Chen, Biprateep Dey, Aishik Ghosh, Michael Kagan, Brian Nord, <b> Nesar Ramachandra </b>; Interpretable Uncertainty Quantification in AI for HEP, Submitted to the Proceedings of the US Community Study on the Future of Particle Physics (Snowmass 2021)'
---


Summary: Estimating uncertainty is at the core of performing scientific measurements in HEP: a measurement is not useful without an estimate of its uncertainty. The goal of uncertainty quantification (UQ) is inextricably linked to the question, "how do we physically and statistically interpret these uncertainties?" The answer to this question depends not only on the computational task we aim to undertake, but also on the methods we use for that task. For artificial intelligence (AI) applications in HEP, there are several areas where interpretable methods for UQ are essential, including inference, simulation, and control/decision-making. There exist some methods for each of these areas, but they have not yet been demonstrated to be as trustworthy as more traditional approaches currently employed in physics (e.g., non-AI frequentist and Bayesian methods). Shedding light on the questions above requires additional understanding of the interplay of AI systems and uncertainty quantification. We briefly discuss the existing methods in each area and relate them to tasks across HEP. We then discuss recommendations for avenues to pursue to develop the necessary techniques for reliable widespread usage of AI with UQ over the next decade.
