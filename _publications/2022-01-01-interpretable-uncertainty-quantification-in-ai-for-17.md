---
title: "Interpretable uncertainty quantification in AI for HEP"
collection: publications
permalink: /publication/2022-interpretable-uncertainty-quantification-in-ai-for
excerpt: '[<u><span style="color:blue">Google Scholar</span></u>](https://scholar.google.com/scholar?q=Interpretable+uncertainty+quantification+in+AI+for+HEP)'
date: 2022-01-01
venue: 'arXiv preprint arXiv:2208.03284'
paperurl: 'https://arxiv.org/abs/2208.03284'
citation: 'Thomas Y Chen and Biprateep Dey and Aishik Ghosh and Michael Kagan and Brian Nord and Nesar Ramachandra (2022). "Interpretable uncertainty quantification in AI for HEP". arXiv preprint arXiv:2208.03284.'
---

Estimating uncertainty is at the core of performing scientific measurements in HEP: a measurement is not useful without an estimate of its uncertainty. The goal of uncertainty quantification (UQ) is inextricably linked to the question, "how do we physically and statistically interpret these uncertainties?" The answer to this question depends not only on the computational task we aim to undertake, but also on the methods we use for that task. For artificial intelligence (AI) applications in HEP, there are several areas where interpretable methods for UQ are essential, including inference, simulation, and control/decision-making. There exist some methods for each of these areas, but they have not yet been demonstrated to be as trustworthy as more traditional approaches currently employed in physics (e.g., non-AI frequentist and Bayesian methods). Shedding light on the questions above requires additional understanding of the interplay of AI systems and uncertainty quantification. We briefly discuss the existing methods in each area and relate them to tasks across HEP. We then discuss recommendations for avenues to pursue to develop the necessary techniques for reliable widespread usage of AI with UQ over the next decade.
