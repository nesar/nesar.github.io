---
title: "AstroMLab 1: Who wins astronomy jeopardy!?"
collection: publications
permalink: /publication/2025-astromlab-1-who-wins-astronomy-jeopardy
excerpt: '[<u><span style="color:blue">Google Scholar</span></u>](https://scholar.google.com/scholar?q=AstroMLab+1:+Who+wins+astronomy+jeopardy!?)'
date: 2025-01-01
venue: 'Astronomy and Computing'
paperurl: 'https://www.sciencedirect.com/science/article/pii/S2213133724001082'
citation: 'Y-S Ting and T Dung Nguyen and Tirthankar Ghosal and Rui Pan and Hardik Arora and Zechang Sun and Tijmen de Haan and Nesar Ramachandra and Azton Wells and Sandeep Madireddy and Alberto Accomazzi (2025). "AstroMLab 1: Who wins astronomy jeopardy!?". Astronomy and Computing.'
---

We present a comprehensive evaluation of proprietary and open-weights large language models using the first astronomy-specific benchmarking dataset. This dataset comprises 4,425 multiple-choice questions curated from the Annual Review of Astronomy and Astrophysics, covering a broad range of astrophysical topics.1 Our analysis examines model performance across various astronomical subfields and assesses response calibration, crucial for potential deployment in research environments. Claude-3.5-Sonnet outperforms competitors by up to 4.6 percentage points, achieving 85.0% accuracy. For proprietary models, we observed a universal reduction in cost every 3-to-12 months to achieve similar score in this particular astronomy benchmark. open-weights models have rapidly improved, with LLaMA-3-70b (80.6%) and Qwen-2-72b (77.7%) now competing with some of the best proprietary models. We â€¦
