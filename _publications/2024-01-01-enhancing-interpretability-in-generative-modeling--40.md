---
title: "Enhancing Interpretability in Generative Modeling: Disentangled Latent Spaces in Scientific Datasets"
collection: publications
permalink: /publication/2024-enhancing-interpretability-in-generative-modeling-
excerpt: '[<u><span style="color:blue">Google Scholar</span></u>](https://scholar.google.com/scholar?q=Enhancing+Interpretability+in+Generative+Modeling:+Disentangled+Latent+Spaces+in+Scientific+Datasets)'
date: 2024-01-01
venue: 'Authorea Preprints'
paperurl: 'https://advance.sagepub.com/doi/full/10.22541/essoar.172926700.08399445'
citation: 'Arkaprabha Ganguli and Nesar Ramachandra and Julie Bessac and Emil Constantinescu (2024). "Enhancing Interpretability in Generative Modeling: Disentangled Latent Spaces in Scientific Datasets". Authorea Preprints.'
---

Summary: Disentangling latent representations is crucial for improving the interpretability and robustness of AI models, especially in complex scientific datasets where domain scientists often know some generative factors, but many others remain unknown. Our method, the Auxiliary information guided Variational AutoEncoder (Aux-VAE), focuses on disentangling the latent space with respect to the known factors while allowing the remaining latent dimensions to jointly capture the unknown factors. This approach not only maintains data reconstruction accuracy but also enhances the interpretability of latent spaces, providing valuable insights into the underlying mechanisms of data generation.
